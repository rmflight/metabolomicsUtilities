---
title: 'Examining Corresponding, Ranked Peaks'
author: 'Robert M Flight'
date: '`r Sys.time()`'
output:
  pdf_document:
    extra_dependencies: ['longtable', 'float']
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## Purpose

Hunter has this idea that we can improve sample-to-sample normalization using relative ranking of peaks based on their intensity ranks.
So this is me trying to figure out if that will work.

## Setup

```{r setup_packages}
library(dplyr)
library(visualizationQualityControl)
library(smirfeTools)
library(ggplot2)
theme_set(cowplot::theme_cowplot())
```

## Load Data

Data we have is **all** the peaks from each sample, along with their ranks, along with the assigned and voted on peaks across samples.
The peaks and their IDs (`Sample_Peak`) should match up between the two data sets.

```{r load_data}
all_peaks = readRDS("testing_stuff/lung_all_peaks.rds")
voted_data = readRDS("testing_stuff/lung_voted_all.rds")
imf_data = extract_imf_emf_data(voted_data, by = "IMF")
```

## Analysis

```{r peak_correspondence}
imf_peaks = imf_data$peaks
peak_counts = apply(imf_peaks, 1, function(.x){sum(!is.na(.x))})
n_sample = ncol(imf_peaks)

peak_frac = peak_counts / n_sample
peak_frac_df = data.frame(imf = rownames(imf_peaks), fraction = peak_frac,
                          stringsAsFactors = FALSE)

imf_2_peak = purrr::map_df(rownames(imf_peaks), function(in_row){
  data.frame(imf = in_row,
             Sample_Peak = as.vector(imf_peaks[in_row, ]),
             stringsAsFactors = FALSE)
})
imf_2_peak = dplyr::filter(imf_2_peak, !is.na(Sample_Peak))
imf_2_peak = dplyr::left_join(imf_2_peak, peak_frac_df, by = "imf")
use_imf = peak_frac >= 0.9
use_peaks = imf_peaks[use_imf, ]
```

```{r peak_ranks}
n_peak_sample = dplyr::group_by(all_peaks, Sample) %>%
  dplyr::summarise(n_peak = dplyr::n())
all_peaks = dplyr::left_join(all_peaks, n_peak_sample, by = "Sample")
all_peaks$fractional_rank = 1 - (all_peaks$rank / all_peaks$n_peak)

imf_ranks = dplyr::left_join(imf_2_peak, all_peaks[, c("Sample_Peak", "fractional_rank", "n_peak")], by = "Sample_Peak")

imf_median_rank = dplyr::group_by(imf_ranks, imf) %>%
  dplyr::summarize(median_rank = median(fractional_rank))
imf_median_rank = dplyr::left_join(imf_median_rank, peak_frac_df, by = "imf")
```

Let's see if the data that Hunter would want to use even exists!

```{r check_fractions_vs_rank}
ggplot(imf_median_rank, aes(x = median_rank, y = fraction)) + geom_point(alpha = 0.5) +
  geom_vline(xintercept = 0.5, color = "red") +
  geom_hline(yintercept = 0.8, color = "red") +
  labs(subtitle = "Median Rank across Samples vs Fraction of Total Samples",
       x = "Median Rank of Peak", y = "Fraction of Samples Present")
```

OK, I previously messed up the rank within a sample, in that a lower number previously meant it was more highly ranked.
**Now it is right!**
A higher number in a sample means it is more highly ranked in the sample.

But we are going to try anyway.
The next step then is to look at the peaks that appear in a decent number of samples, say **10** (so a fraction >= 0.05), and let's slice it down to things that are in the range of 0.5 median ranked across samples, using a range of 0.4 to 0.6.

```{r plot_points_inrange}
ggplot(imf_median_rank, aes(x = median_rank, y = fraction)) + geom_point(alpha = 0.5) +
  xlim(0.4, 0.6) + 
  geom_hline(yintercept = 0.05, color = "red") +
  labs(subtitle = "Median Rank across Samples vs Fraction of Total Samples",
       x = "Median Rank of Peak", y = "Fraction of Samples Present")
```

```{r trim_data}
trim_ranks = dplyr::filter(imf_median_rank, dplyr::between(median_rank, 0.4, 0.6),
                           fraction >= 0.055)

use_peaks = dplyr::filter(imf_2_peak, imf %in% trim_ranks$imf) %>%
  dplyr::left_join(., all_peaks, by = "Sample_Peak")

ggplot(use_peaks, aes(x = n_peak, y = fractional_rank)) + geom_point(alpha = 0.5) +
  geom_smooth(method = "lm")
cor.test(use_peaks$n_peak, use_peaks$fractional_rank)
```

OK, there is definitely a positive correlation between the fractional rank of a peak and the number of peaks in the sample.
Now, can we correct it and make the correlation value lower?

```{r correct_data}
max_peaks = max(use_peaks$n_peak)

use_peaks2 = dplyr::select(use_peaks, imf, Sample_Peak, fraction, rank, n_peak, fractional_rank) %>%
  dplyr::mutate(peak_max = n_peak / max_peaks)
use_peaks2 = dplyr::mutate(use_peaks2, corrected_rank = fractional_rank / (peak_max))

ggplot(use_peaks2, aes(x = n_peak, y = corrected_rank)) + geom_point(alpha = 0.5) +
  geom_smooth(method = "lm")
cor.test(use_peaks2$n_peak, use_peaks2$corrected_rank)
```

OK, this doesn't work, and I think I know why.
It's because the the maximum value of `fractional_rank` is **not** 1.
If the maximum values **was** 1, then this simple correction would work.

An alternative method is to use the fitted linear model, and the ratio of the current predicted value to the maximum predicted value.
I think that should work to transform our correlation to something closer to 0.

```{r correct_try2}
fit_npeak = lm(fractional_rank ~ n_peak, data = use_peaks2)
fitted_ranks = fit_npeak$fitted.values
use_peaks2$rank_ratio = fitted_ranks / max(fitted_ranks)
use_peaks2 = dplyr::mutate(use_peaks2, corrected_rank2 = fractional_rank / rank_ratio)

ggplot(use_peaks2, aes(x = n_peak, y = corrected_rank2)) + geom_point(alpha = 0.5) +
  geom_smooth(method = "lm")
cor.test(use_peaks2$n_peak, use_peaks2$corrected_rank2)
```

**And the correlation is gone!** The 95% CI on the test is now: -0.02 - 0.02.

## How Does This Affect Normalization?

OK, we can correct the ranking of peaks within a sample.
So how does this affect normalization?

If we correct the ranking of **all** peaks in **all** samples, then we have a much better idea of **which** peaks actually have ranks close to **0.5** in a majority of samples, if they exist.

We should be using peaks with a median rank close to **0.5** to normalize samples.
We can test this by doing a p-value comparison, similar to what we've done previously (code which I need to find).